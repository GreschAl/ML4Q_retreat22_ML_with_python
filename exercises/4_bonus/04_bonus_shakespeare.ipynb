{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdUIvZ9w_K7L"
   },
   "source": [
    "# Notebook 4 (bonus): Shakespeare Generator\n",
    "\n",
    "In this exercise we aim to compose text just like Shakespeare would do today. To this end, the network takes a text file as input and is trained to predict the next character in a sequence. In this case the input data (input.txt) is part of Shakespeares \"The Taming of the Shrew\". The network is used to generate new data that is similar to the given input data. After some training the generated text should look like a real Shakespearean artwork. A detailed description can be found here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIr3Y7ZI_K7V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6aKqY-E_K7c"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Gka8BB9_dbF",
    "outputId": "73fd2c13-2990-4d1e-8086-b8baab6bc78f"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "!git clone https://github.com/GreschAl/ML4Q_retreat22_ML_with_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hcl698CX_di_"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "folder = \"/content/ML4Q_retreat22_ML_with_python/exercises/4_bonus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3COVzDF_K7g",
    "outputId": "792f19c2-9c4e-48ca-f898-876b2d6431f4"
   },
   "outputs": [],
   "source": [
    "# data I/O\n",
    "data = open(folder+'input.txt', 'r').read()  # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtJjdwED_K7n",
    "outputId": "905b5295-8325-4623-947c-4c1755189819"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "# hyperparameters\n",
    "hidden_size = 100  # size of hidden layer of neurons\n",
    "seq_length = 25  # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "optimizer_func = torch.optim.Adagrad\n",
    "\n",
    "# model\n",
    "model = nn.LSTM(vocab_size,hidden_size)\n",
    "linear = nn.Linear(hidden_size,vocab_size)\n",
    "print(model)\n",
    "print(linear)\n",
    "\n",
    "# optimizers\n",
    "opt_model = optimizer_func(model.parameters(),lr=learning_rate)\n",
    "opt_lin   = optimizer_func(linear.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdX_QxtT_K7q"
   },
   "source": [
    "Helper function to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEMTWyra_K7s"
   },
   "outputs": [],
   "source": [
    "def lossFun(model, linear, inputs, targets, cprev, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is 1xH array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, cs, hs, caches, ys, ps = {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = hprev\n",
    "    cs[-1] = cprev\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        ######################################################################\n",
    "        ### TODO: ###\n",
    "        ######################################################################\n",
    "        # encode in 1-of-k representation\n",
    "        # xs[t] = ...  \n",
    "        # put xs through LSTM\n",
    "        # ...\n",
    "        # put output through dense layer to obtain unnormalized log probabilities for next chars\n",
    "        # ys[t] = ...\n",
    "        ######################################################################\n",
    "        ps[t] = torch.softmax(ys[t],dim=1)  # probabilities for next chars\n",
    "        loss += F.cross_entropy(ys[t],targets[t].view(-1))  # cross-entropy loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp-EfSJW_K7w"
   },
   "source": [
    "Helper function to create new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WKpIjQP_K7z"
   },
   "outputs": [],
   "source": [
    "def sample(model, linear, c, h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = torch.zeros((1,vocab_size),dtype=torch.float)\n",
    "    x[0,seed_ix] = 1\n",
    "    ixes = []\n",
    "    with torch.no_grad():\n",
    "        for t in range(n):\n",
    "            ######################################################################\n",
    "            ### TODO: ###\n",
    "            ######################################################################\n",
    "            # repeat procedure from before\n",
    "            # p = ... probabilities for next character\n",
    "            # ix = ... draw new char from distribution given by p\n",
    "            x = torch.zeros((1,vocab_size),dtype=torch.float)\n",
    "            x[0,ix] = 1\n",
    "            ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUCxPwYp_K73"
   },
   "source": [
    "Endless loop of training and printing samples every 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neMsktet_K74",
    "outputId": "477bf075-fb24-4387-a2a5-7a8c593121e8"
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        cprev = torch.zeros((1, hidden_size))\n",
    "        hprev = torch.zeros((1, hidden_size))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]],dtype=torch.long)\n",
    "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]],dtype=torch.long)\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(model, linear, cprev, hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss = lossFun(model, linear, inputs, targets, cprev, hprev)\n",
    "    # perform parameter update with optimizers\n",
    "    opt_model.zero_grad()\n",
    "    opt_lin.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_model.step()\n",
    "    opt_lin.step()\n",
    "    \n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress    \n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoa-ATtT_K77"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_bonus_shakespeare.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
