{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xIWLMa3-tzS"
   },
   "source": [
    "# Notebook 3: Reinforcement Learning\n",
    "\n",
    "**Authors:** Course material from Shimon Whiteson. Introduction to reinforcement learning, 2019 (https://github.com/mlss-skoltech/tutorials_week2).\n",
    "\n",
    "Adapted for the ML4Q-retreat 2022 by Alexander Gresch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxcyIEtVLyv_",
    "outputId": "0a6860d5-be2f-40c7-da4c-8305ba68f33a"
   },
   "outputs": [],
   "source": [
    "!pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6vaPRyIKKwj"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3gU0oNyKK1H"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U3DD17u-tza"
   },
   "source": [
    "## Dyna-Q: RL with action values\n",
    "\n",
    "In this exercise you will implement **Dyna-Q**, which is a model-based Q-planning algorithm (planning with action values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_b_jTXmAAPJ"
   },
   "source": [
    "## Cliff walking Environment\n",
    "\n",
    "In this exercise we will work with the Cliff walking environment from OpenAI's gym library.  \n",
    "Make yourself familiar with the environment:\n",
    "- https://www.gymlibrary.dev/environments/toy_text/cliff_walking/\n",
    "\n",
    "The environment also provides some useful attributes:\n",
    "- ```env.nS```: the number of states. The env sets up a grid of shape (4x12) s.t. there are 48 distinct states.\n",
    "- ```env.nA```: the number of actions. The agent can decide to move to adjacent grid places, but not vertically. This makes up 4 possible movements.\n",
    "    \n",
    "Each time we start anew, we have to reset the environment via\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "Note that this yields an intial state that is presented to the agent.\n",
    "Upon choosing an action, the environment provides the agent with\n",
    "\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "where ```done``` is a boolean that flags whether we have reached the terminal state. ```info``` may contain further information about the environment (which is not presented to the agent, however).\n",
    "\n",
    "Your task (and the task of RL in general) is to tune an algorithm that picks the sequence of actions that maximize the (discounted) return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c474_g8q-tzo",
    "outputId": "9f33b292-cc77-4a8e-c26b-94896f959b3e"
   },
   "outputs": [],
   "source": [
    "# let's initialize the environment\n",
    "env = gym.make(\"CliffWalking-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv1A9mAg-tzr"
   },
   "source": [
    "Print the observation space and action space of the environment to see if it matches the description above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh-7n0A2-tzt",
    "outputId": "cfff9c6c-55f5-46bf-8067-a38c8fffd73d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyNG-dIZID3q"
   },
   "source": [
    "## Policies\n",
    "\n",
    "In our setup, policies are functions that take two arguments, ```env``` and ```state```, and return an action based on that state:\n",
    "\n",
    "```\n",
    "def my_policy(env, state):\n",
    "    action = ...\n",
    "    return action\n",
    "```\n",
    "\n",
    "Below we implemented a function that runs one rollout of a given policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m90OxfWUIBe7"
   },
   "outputs": [],
   "source": [
    "def rollout(env, policy, max_steps=100, render=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.\n",
    "    num_steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        action = policy(env, state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        num_steps += 1\n",
    "        done = done or num_steps > max_steps\n",
    "        \n",
    "        if render:\n",
    "            sleep(0.4)\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtYl4r-3IKZ9"
   },
   "source": [
    "And another function that runs multiple rollouts of a given policy and averages the total rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H_vx_oGIH4_"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, policy, num_rollouts=100, max_steps=100):\n",
    "    return sum(rollout(env, policy, max_steps=max_steps) for _ in range(num_rollouts)) / num_rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcWYPzfcIOlZ"
   },
   "source": [
    "### Random policy\n",
    "The random policy selects random actions and ignores the current state.  \n",
    "Implement this policy by selecting the next action at random, regardless of the given state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8u9esxWINyU"
   },
   "outputs": [],
   "source": [
    "def random_policy(env, state):\n",
    "    ################################\n",
    "    ### TODO: ###\n",
    "    ################################\n",
    "    # return a valid action at random\n",
    "    # action = ...\n",
    "    ################################\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5C3BdPx_IRff",
    "outputId": "75a64a18-6c74-4dee-f40f-a804417ffc42"
   },
   "outputs": [],
   "source": [
    "# have a look how the random policy performs over 20 steps\n",
    "total_reward = rollout(env, random_policy, max_steps=20, render=True)\n",
    "print('\\nTotal reward:', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDh6_LXQMJ0F"
   },
   "source": [
    "Unfortunately, the rendering does not work with google colab :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRq87nBfMTvn",
    "outputId": "0526e977-8e46-4768-b162-97c7406532f4"
   },
   "outputs": [],
   "source": [
    "# without rendering, it is very fast to evaluate the average performance of the same policy over 20 steps as\n",
    "print('Average total reward:', evaluate(env, random_policy, max_steps=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo9Yrt4P-tz4"
   },
   "source": [
    "## Implement Dyna-Q\n",
    "Implement the Tabular Dyna-Q algorithm described in Chapter 8.2 of [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 164. (You do not need to look up the original source. The book is nevertheless a very valuable ressource if you want to dive deeper into the field of RL.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m7mLsb4-tz4"
   },
   "source": [
    "![](http://incompleteideas.net/book/first/ebook/pseudotmp18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "- we initialize all $Q(s,a) = 0$\n",
    "- in practice, $Q$ will be a dictionary with keys $s$ and corresponding lists as items for all possible actions $a$. This means that $Q[s][a] \\equiv Q(s,a)$\n",
    "- with Model(s,a), we simply store previously observed (state,action,new_state,reward) combinations such that we can reuse them later to train the agent more efficiently. This is done for step (f)\n",
    "- instead of \"forever\", we preset a fixed number of episodes instead. Your time is valuable, I understand this :)\n",
    "- in step (f), we only use Model(s,a) to further adjust the Q-values. We do NOT alter the current state of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRD_x0uP-tz4"
   },
   "source": [
    "Dyna-Q uses the an $\\epsilon$-greedy action selection and Q update formula (in step (d)).\n",
    "Implement those two first. I also provide you with the function argmax_random that selects randomly from those indices for which x is maximum. This prevents breaking ties for multiple maxima at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvsezvdX-tz5"
   },
   "outputs": [],
   "source": [
    "def argmax_random(x):\n",
    "    return np.random.choice(np.flatnonzero(x == x.max()))\n",
    "\n",
    "def epsilon_greedy_action(env, Q, epsilon, state):\n",
    "    ##################################################\n",
    "    ### TODO: ###\n",
    "    ##################################################\n",
    "    # with probability epsilon, follow a random policy,\n",
    "    # otherwise, pick one of the argmax of Q[state]\n",
    "    # action = ...\n",
    "    ##################################################\n",
    "    return action\n",
    "\n",
    "def update_Q(Q, old_state, new_state, action, reward, alpha, gamma=0.95):\n",
    "    ##################################################\n",
    "    ### TODO: ###\n",
    "    ##################################################\n",
    "    # implement step (d)\n",
    "    # Q[...] = ...\n",
    "    ##################################################\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3hoh93K-tz5"
   },
   "source": [
    "Now implement the main part of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYFO0dZd-tz6"
   },
   "outputs": [],
   "source": [
    "def train_dyna_Q(env, num_episodes, n=0, alpha=0.1, epsilon=0.1, silent=False):\n",
    "    \"\"\"Estimate optimal action-value function using the Dyna-Q algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment to train on.\n",
    "        num_episodes: Number of episodes to run.\n",
    "        n: The number of planning steps in Dyna-Q\n",
    "        alpha: Step size for the action-value update.\n",
    "        epsilon: Parameter for the epsilon greedy action selection.\n",
    "    \n",
    "    Returns:\n",
    "        Q: The finished Q-table.\n",
    "        num_steps: Array containing number of steps per episode.\n",
    "        episode_rewards: Array containing the sum of rewards of the episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize Dyna-Q variables\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) # this sets all action values to zero\n",
    "    model = {}\n",
    "    \n",
    "    num_steps = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        if not silent:\n",
    "            print(\"\\rEpisode {}/{}\".format(episode, num_episodes), flush=True, end='')\n",
    "        \n",
    "        state = env.reset() # this already accomplishes step (a)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        cumulated_reward = 0\n",
    "        while not done:\n",
    "            # step (b) to (f)\n",
    "            ##################################################\n",
    "            ### TODO: ###\n",
    "            ##################################################\n",
    "            # implement step (b)\n",
    "            # ...\n",
    "            # implement step (c)\n",
    "            # ...\n",
    "            # step (d)\n",
    "            # ...\n",
    "            # step (e)\n",
    "            # ...\n",
    "            # finally, step (f). Take n samples stored in model at random to update Q further\n",
    "            # ...\n",
    "            ##################################################\n",
    "            # we also keep track of the total number of steps and all rewards gained so far\n",
    "            steps += 1\n",
    "            cumulated_reward += reward\n",
    "            \n",
    "        \n",
    "        num_steps.append(steps)\n",
    "        episode_rewards.append(cumulated_reward)\n",
    "    \n",
    "    return Q, np.array(num_steps), np.array(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITkNKrsl-tz7"
   },
   "source": [
    "Let's test your implementation! Choose at least 200 episodes with a small number of replays $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4mllSIc-tz8",
    "outputId": "f469c912-7bad-4061-b1dd-19f0d1a93d55"
   },
   "outputs": [],
   "source": [
    "# change these to your liking\n",
    "num_episodes = 200\n",
    "num_replays  = 5\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\").env\n",
    "Q, num_steps, rewards = train_dyna_Q(env, num_episodes, num_replays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot how num_steps and rewards changes during training of the agent. To make the curves a bit smoother, we use an moving average over some recent episodes. The black dotted line corresponds to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, window_size):\n",
    "    return np.convolve(x, np.ones(window_size), 'valid') / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_steps,alpha=0.5)\n",
    "plt.plot(moving_average(num_steps,10),\"r-\")\n",
    "plt.hlines(13,0,num_episodes,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylim(12,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the negative reward (again with a moving average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(-rewards,alpha=0.5)\n",
    "plt.plot(moving_average(-rewards,10),\"r-\")\n",
    "plt.hlines(13,0,num_episodes,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylim(0,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8EIKZGp-tz8"
   },
   "source": [
    "After training, your agent should find the goal easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32Lm2toI-tz9",
    "outputId": "16ad5bfd-fd9c-4ff8-c58d-09839c76c522"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(100):\n",
    "    action = epsilon_greedy_action(env, Q, 0.3, state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Walk stopped after {} steps.\".format(i+1))\n",
    "        break\n",
    "    sleep(40e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eokIAz6aWDqB"
   },
   "source": [
    "The optimal number of steps is 13. Why is your agent performing worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frNN0UGs-tz9"
   },
   "source": [
    "## Key parameters in Dyna-Q\n",
    "\n",
    "Now you will explore the role of the key parameters $n$ and $\\alpha$. \n",
    "\n",
    "#### Exploring $n$\n",
    "In particular, plot the two curves from above averaged over 10 runs for $n \\in \\{0,5,50\\}$. Keep the number of episodes fixed to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB-zok_J-tz-",
    "outputId": "d424d896-120c-4e38-e610-9fcaea24d5be"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# measures the execution time\n",
    "# first collect the data for plotting\n",
    "##################################################\n",
    "### TODO: ###\n",
    "##################################################\n",
    "# define various values for n and train the agent 10 independent times on 50 episodes each.\n",
    "# Keep track on the average num_steps and the average cumulative reward over the 10 runs.\n",
    "# no_eps = ... counts the average number of steps averaged over 10 runs per n-value and episode\n",
    "# no_rewards = ... counts the average cumulative reward averaged over 10 runs per n-value and episode\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "ya_hDcUa-tz-",
    "outputId": "651e2347-43fc-4fec-a203-6660106e399b"
   },
   "outputs": [],
   "source": [
    "# then plot the data for the number of steps\n",
    "plt.figure()\n",
    "for i,n in enumerate(n_vals):\n",
    "    plt.plot(np.arange(1,51),no_eps[i],label=\"n = {} \".format(n))\n",
    "plt.hlines(13,1,50,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.ylim(10,300)\n",
    "plt.xlim(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then plot the data for the cumulative rewards\n",
    "plt.figure()\n",
    "for i,n in enumerate(n_vals):\n",
    "    plt.plot(np.arange(1,51),no_rewards[i],label=\"n = {} \".format(n))\n",
    "plt.hlines(-13,1,50,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.ylim(-300,-10)\n",
    "plt.xlim(1,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh2yVpLL-tz-"
   },
   "source": [
    "#### Exploring $\\alpha$\n",
    "Now also plot the learning curves averaged over 10 runs for $\\alpha \\in \\{0.0625, 0.25, 0.5, 1.0\\}$. Keep $n=5$ as a result of the previous plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDcrbvZx-tz_",
    "outputId": "7e5db003-7c03-43e7-de9a-35bea8112d69",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# measures the execution time\n",
    "# first collect the data for plotting\n",
    "##################################################\n",
    "### TODO: ###\n",
    "##################################################\n",
    "# repeat the previous task with varying alpha values\n",
    "# no_eps_alpha = ...\n",
    "# no_rewards_alpha = ...\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "SLJ6M9T5-t0A",
    "outputId": "0b9a21b0-225e-4c41-a8b9-3d30387fb521"
   },
   "outputs": [],
   "source": [
    "# then plot the data\n",
    "plt.figure()\n",
    "for i,alpha in enumerate(alpha_vals):\n",
    "    plt.plot(np.arange(1,51),no_eps_alpha[i],label=\"alpha = {} \".format(alpha))\n",
    "plt.legend()\n",
    "plt.hlines(13,1,50,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.xlim(1,51)\n",
    "plt.ylim(10,60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIylNeXxUTvL"
   },
   "outputs": [],
   "source": [
    "# then plot the data for the cumulative rewards\n",
    "plt.figure()\n",
    "for i,n in enumerate(n_vals):\n",
    "    plt.plot(np.arange(1,51),no_rewards_alpha[i],label=\"n = {} \".format(n))\n",
    "plt.hlines(-13,1,50,colors=\"black\",linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.ylim(-300,-10)\n",
    "plt.xlim(1,50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "03_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
