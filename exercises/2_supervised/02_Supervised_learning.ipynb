{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "possible-truck",
   "metadata": {
    "id": "possible-truck"
   },
   "source": [
    "# Notebook 2: Classification with Neural Networks\n",
    "\n",
    "**Authors:** Kenny Choo, Mark H. Fischer, Eliska Greplova for the conference \"Summer School: ML in Quantum Physics and Chemistry\" (24.08.-03.09.2021, Warsaw)\n",
    "\n",
    "Adapted for the ML4Q-retreat 2022 by Alexander Gresch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9db9b",
   "metadata": {},
   "source": [
    "## Import of the first ML library!\n",
    "There are various libraries useful for ML practitioners. Among the most popular are Tensorflow (with Keras) and PyTorch. Both are pretty competitive with each other, so basically both offer the same functionalities [[source]](https://www.imaginarycloud.com/blog/pytorch-vs-tensorflow/).\n",
    "- **Tensorflow** is a Python numerical library (created by the Google Brain team) with many methods being in fact high-performance C++ binaries. It can be accelerated in the Google Colaboratory by choosing TPU (Tensor Programming Units). Keras is the high-level API of TensorFlow 2, so a kind of user-friendly overlay which simplifies many operations.\n",
    "- **PyTorch** is a Python library created by Facebook AI Research Lab. Before Tensorflow 2.0, PyTorch strength was a little different way of storing and building ML models, which allowed easier interaction with the models' internals. But Tensorflow 2.0 adapted to that and now the main difference is a little easier debugging of a PyTorch code than Tensorflow one.\n",
    "\n",
    "Fun fact to confuse you even more: Keras API used to build ML models in Tensorflow looks very similar to the PyTorch way of building ML models. We will stick to PyTorch. Note that there exists a wrapper module, pytorch lightning, for a more convenient control of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the pytorch lightning wrapper locally for this notebook\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44K0-SShu7X3",
   "metadata": {
    "id": "44K0-SShu7X3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Helper Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rYL08R8jrUpF",
   "metadata": {
    "id": "rYL08R8jrUpF"
   },
   "source": [
    "## Step 0. Loading and saving files with Google Colaboratory\n",
    "\n",
    "Within these tutorials, we will need to upload some data (in particular, training data). E.g., today, we will upload Monte Carlo generated configurations of Ising spins on a 30x30 lattice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A0Bc_uCRPsfG",
   "metadata": {
    "id": "A0Bc_uCRPsfG"
   },
   "source": [
    "- *Clone the GitHub repository*\n",
    "\n",
    "If you want to avoid working with Google drive, you can use this work-around. All needed data are on [this GitHub](https://github.com/Shmoo137/SummerSchool2021_MLinQuantum). You can clone this GitHub repository into your Colab environment in the same way as you would in your local machine, using git clone. Once the repository is cloned, refresh the file-explorer on the left to browse through its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DtMKirDKQLW3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtMKirDKQLW3",
    "outputId": "952d3967-983d-4eb8-f64d-604e9ea90ccc"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "!git clone https://github.com/GreschAl/ML4Q_retreat22_ML_with_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgorAcwwQwZJ",
   "metadata": {
    "id": "mgorAcwwQwZJ"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "folder = \"/content/ML4Q_retreat22_ML_with_python/exercises/Ising_data\"\n",
    "# Access the data with simple commands:\n",
    "ising_training_configs = np.load(folder + \"/ising_training_configs_30x30.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6L_cH6L3QL54",
   "metadata": {
    "id": "6L_cH6L3QL54"
   },
   "source": [
    "Take note these are local and temporary files, and will disappear after closing the notebook. \n",
    "\n",
    "In this notebook we will repeat the exercise from the notebook `01_Unsupervised_learning` on using data analysis to try to identify different phases of matter. We have seen that sometimes clustering can be really powerful tool, but sometimes it does not work too well. Here, we will learn how to accomplish the same task using neural networks and see if they can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-minutes",
   "metadata": {
    "id": "reserved-minutes"
   },
   "source": [
    "# Example #1: Ising Spin Configuration Classification\n",
    "\n",
    "\n",
    "The Ising model is given by the (classical) Hamiltonian:\n",
    "\n",
    "\\begin{align}\n",
    "H(\\boldsymbol{\\sigma}) = -\\sum_{<ij>} \\sigma_{i}\\sigma_{j},\n",
    "\\end{align}\n",
    "where the spins $\\sigma_{i} \\in \\lbrace -1, 1 \\rbrace$ are binary variables living on the vertices of a square lattice and the sum is taken over nearest neighbours $<ij>$. We have set $J=1$.\n",
    "  \n",
    "At a given temperature $\\beta = 1/T$, the probability of a configuration $\\sigma$ is given by the Boltzmann distribution\n",
    "  \n",
    "\\begin{align}\n",
    "  P(\\boldsymbol{\\sigma}) = \\frac{e^{-\\beta H(\\boldsymbol{\\sigma})}}{Z},\n",
    " \\end{align}\n",
    "  \n",
    "  where $Z$ is the partition function. This model exhibits a phase transition from the ferromagnetic phase at low tempertures to a paramagnetic phase at high temperatures. The transition temperature is $T_c \\approx 2.2692$.\n",
    "  \n",
    "  **Task**\n",
    " \n",
    "1.   Classify the ferromagnetic versus the paramagnetic phase of the Ising model\n",
    "2.   Find the transition temperature\n",
    "  \n",
    "**Dataset**: Monte Carlo generated configurations on a 30x30 square lattice. The configuration are labelled by temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-norway",
   "metadata": {
    "id": "above-norway"
   },
   "source": [
    "## Step 1: Import data and analyze the data shape\n",
    "\n",
    "The folder `Ising_data` contains Monte Carlo generated Ising configurations on the two-dimensional lattice. The data set is divided into training and test parts and corresponding label files containing the temperature, $T$, of each Monte Carlo sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-stylus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "outstanding-stylus",
    "outputId": "a8643e4c-af7e-456b-8524-495eb81eeaac"
   },
   "outputs": [],
   "source": [
    "N = 30 # linear dimension of the lattice \n",
    "\n",
    "ising_training_configs = np.load(folder + \"/ising_training_configs_{0}x{0}.npy\".format(N))\n",
    "ising_training_labels = np.load(folder + \"/ising_training_labels_{0}x{0}.npy\".format(N))\n",
    "ising_test_configs = np.load(folder + \"/ising_test_configs_{0}x{0}.npy\".format(N))\n",
    "ising_test_labels = np.load(folder + \"/ising_test_labels_{0}x{0}.npy\".format(N))\n",
    "\n",
    "print('train_images.shape =', ising_training_configs.shape)\n",
    "print('train_labels.shape =', ising_training_labels.shape)\n",
    "print('test_images.shape =', ising_test_configs.shape)\n",
    "print('test_labels.shape =', ising_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-economy",
   "metadata": {
    "id": "sharp-economy"
   },
   "source": [
    "We see that we have a training set of size 1000 and a test set of size 1000.\n",
    "Each image is a 30x30 array which takes values in {-1, 1}. The labels of these images are the temperatures and they are in [1, 3.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-shock",
   "metadata": {
    "id": "foreign-shock"
   },
   "source": [
    "## Step 2: Prepare data\n",
    "\n",
    "At the moment, our configurations are labelled by their temperature. Since we want to learn to classify the two phases, we need to label our data by 'Ordered' (label=0) vs 'Disordered (label=1).\n",
    "\n",
    "Let us assume that we know $1.5=T_{low}<T_{c}< T_{high} = 2.5$. Then we exclude all the data between $T_{low}$ and $T_{high}$. We label all configurations below $T_{low}$ with '0' and all those above $T_{high}$ with '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-swedish",
   "metadata": {
    "id": "delayed-swedish"
   },
   "outputs": [],
   "source": [
    "# Assign labels according to the temperature\n",
    "T_low = 1.5\n",
    "T_high = 2.5\n",
    "\n",
    "########################################################################\n",
    "### TODO: ###\n",
    "########################################################################\n",
    "# Starting with training data, create the labels for the data according\n",
    "# to the explanation above. Do not forget to omit data from inbetween the phases.\n",
    "# train_images = ...\n",
    "# train_labels = ...\n",
    "\n",
    "# do the same for the test data\n",
    "# test_images = ...\n",
    "# test_labels = ...\n",
    "########################################################################\n",
    "\n",
    "# Now you should have smaller training data set, check it:\n",
    "print(train_images.shape,train_labels.shape)\n",
    "# Now you should have smaller training data set, check it:\n",
    "print(test_images.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88dcb3",
   "metadata": {},
   "source": [
    "Before we can commence with setting up the neural networks, we have to wrap the data further into a pytorch data loader instance. For example, the data loader does the correct mini-batching for us. It is also possible, to alter the data as they are being fed into the model. This is REALLY helpful if you are dealing with large data samples such as high-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44736a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(torch.tensor(train_images,dtype=torch.float),torch.tensor(train_labels,dtype=torch.long))\n",
    "test_set  = TensorDataset(torch.tensor(test_images,dtype=torch.float), torch.tensor( test_labels,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b1a53",
   "metadata": {},
   "source": [
    "## Step 3: Setup the model\n",
    "\n",
    "Specifications of our model: \n",
    "\n",
    "*  Our model takes in a 30 by 30 array\n",
    "*  And outputs a 2-dimensional vector\n",
    "\n",
    "The 2-dimensional vector gives the models prediction for whether the system is ferromagnetic (i.e., $T < T_c$) or paramagnetic ($T>T_c$).\n",
    "\n",
    "The model's prediction is given by the index with the largest value, i.e. argmax(output)\n",
    "\n",
    "There are two ways to create models within pytorch.\n",
    "\n",
    "1.   Sequential Model\n",
    "2.   Model class with the functional API\n",
    "\n",
    "In both methods, the basic building block is the layer. A layer takes some input tensor and applies some transformation and returns an output tensor.\n",
    "\n",
    "First let us explore the sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff509ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define a convenience layer that flattens the 2d input samples into a 1d vector\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(len(x),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(900,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,2)\n",
    ")\n",
    "\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8f05d",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Why 32 neurons? Good question! No one knows exactly ;) Building a ML model is a combination of educated guess, good practices, and pure luck. General rule - the simpler model (the smaller number of parameters), the better. We will encounter more quantities on whose values we need to decide to some degree arbitrarily. These quantities are called **hyperparameters**.\n",
    "\n",
    "*Aside note for curious minds:*\n",
    "\n",
    "While training, you optimize the parameters of the model. You check the performance of the trained model (on the subset of data called validation data) and then try to improve it by playing with hyperparameters. You can think that you optimize the model parameters on the training data and the hyperparameters on the validation data. The final test of the model performance is the test data. \n",
    "\n",
    "*Question:* Why shouldn't you play with your hyperparameters to improve your model performance on test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule. The init() takes all relevant model(s), hyperparameters etc.\n",
    "# We only have to define the training_step(), i.e. how a mini-batch of data is processed.\n",
    "# The self.log() functions allows to keep track of the training loss but can also be used for other accuracy metrics.\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, network, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.lr = learning_rate\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        # batch = (x,y) comes internally from the data loader\n",
    "        # batch_idx is only passed for internal reasons\n",
    "        x, y = batch\n",
    "        \n",
    "        scores = self.network(x)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        y_pred = torch.argmax(F.softmax(scores, dim=1), dim=1)\n",
    "        correct = torch.sum((y_pred==y).to(dtype=torch.long))\n",
    "        # Logging to TensorBoard by default\n",
    "        values = {\"loss\": loss, \"train_correct\": correct, \"train_total\": len(y)}  # add more items if needed\n",
    "        self.log_dict(values)\n",
    "        # self.log(\"train_loss\", loss) <- in case you want to only log one value\n",
    "        return values\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        scores = self.network(x)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        y_pred = torch.argmax(F.softmax(scores, dim=1), dim=1)\n",
    "        correct = torch.sum((y_pred==y).to(dtype=torch.long))\n",
    "        values = {\"loss\": loss, \"val_correct\": correct, \"val_total\": len(y)}\n",
    "        self.log_dict(values)\n",
    "        return values\n",
    "\n",
    "    def training_epoch_end(self,outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "        # calculating average loss  \n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "        # calculating correct and total predictions\n",
    "        correct=sum([x[\"train_correct\"] for  x in outputs])\n",
    "        total=sum([x[\"train_total\"] for  x in outputs])\n",
    "\n",
    "        self.train_loss.append(avg_loss)\n",
    "        self.train_acc.append(correct/total)\n",
    "\n",
    "        return\n",
    "\n",
    "    def validation_epoch_end(self,outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "        # calculating average loss  \n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "        # calculating correct and total predictions\n",
    "        correct=sum([x[\"val_correct\"] for  x in outputs])\n",
    "        total=sum([x[\"val_total\"] for  x in outputs])\n",
    "\n",
    "        self.test_loss.append(avg_loss)\n",
    "        self.test_acc.append(correct/total)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return torch.argmax(F.softmax(self.network(batch[0]),dim=1),dim=1)\n",
    "\n",
    "    def predict(self,data):\n",
    "        with torch.no_grad():\n",
    "            return F.softmax(self.network(torch.tensor(data,dtype=torch.float)),dim=1).numpy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = LitModel(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec89a5",
   "metadata": {},
   "source": [
    "We also defined a few specific details before commencing with the training, namely:\n",
    "\n",
    "\n",
    "1.   Loss function: we need to choose what function we want our model to minimise e.g. mean square error or cross entropy or ...\n",
    "2.   Optimisation method: How we want to update the weights e.g. stochastic gradient descent or ADAM or ...\n",
    "3.   Metrics: some quantity we want to keep track off while we are training, e.g. value of the loss function or the accuracy of the model...\n",
    "\n",
    "One could also choose other loss functions or optimisers: https://pytorch.org/docs/stable/nn.html#loss-functions, https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "It is always good to check the history of the training. How did the training loss look like? How did the validation loss look like? You can learn a lot from that and we will show you a very meaningful example in the end of this notebook. For now, remember it is a good practice to plot the training and validation loss!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee496b2f",
   "metadata": {},
   "source": [
    "## Step 4. Train the model\n",
    "\n",
    "Finally we are ready to train our model. Here we basically just need to feed the data set to our model, and the model would minimise the loss function we chose and update the model parameters according to the optimisation method we specified. \n",
    "\n",
    "The last thing we need to choose is the number of epochs and the batch_size.\n",
    "\n",
    "\n",
    "*   batch_size: this is the number of images we feed to our model in 1 iteration\n",
    "*   epochs: the number of times we run through our data set.\n",
    "\n",
    "Let's suppose batch size = 100. Then the training proceeds as follows: \n",
    "\n",
    "1.   Divide our dataset in batches of 100. \n",
    "2.   Take a batch of 100 samples and feed it to the model. This gives 100 output vectors from which we compute the loss function and its gradient w.r.t. to model parameters.\n",
    "3. Use the gradients to update the model parameters according to the optimiser we chose \n",
    "4. Repeat steps 2 and 3 until we have cycled through to the end of the dataset. This will be the end of one epoch.\n",
    "\n",
    "Number of iterations in one epoch = size of data set / batch_size\n",
    "\n",
    "\n",
    "A few comments:\n",
    "\n",
    "* The smaller the batch size, the faster the model trains.\n",
    "* The smaller the batch size, the more noisy the training will be.\n",
    "* Some amount of noise is useful to prevent us from getting stucked in local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bd93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the torch dataset instance in a data loader\n",
    "train_loader = DataLoader(train_set,batch_size=16,shuffle=True)\n",
    "test_loader  = DataLoader(test_set ,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5)\n",
    "trainer.fit(model=dense_model, train_dataloaders= train_loader, val_dataloaders=test_loader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef182617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful plotting procedure to plot training and validation losses vs. training time (measured in so-called epochs)\n",
    "def plot_history(model,key=\"loss\"):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for i, (name, style) in enumerate(zip([\"train\",\"test\"],[\"r-\",\"r--\"])):\n",
    "        data = getattr(model,name+\"_\"+key)[i:]\n",
    "        plt.plot(data, style, label=name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,len(data)])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the training looked like:\n",
    "# summarize history for accuracy\n",
    "plot_history(dense_model, 'acc')\n",
    "# summarize history for loss function value\n",
    "plot_history(dense_model, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e297c9",
   "metadata": {},
   "source": [
    "### Can you tell something out of these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a4257",
   "metadata": {},
   "source": [
    "**Answer**: the training was successful: in both metrics, the graphs for training and validation data are very similar to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a02b35",
   "metadata": {},
   "source": [
    "## Step 5. Evaluate our model\n",
    "Now that our model is trained, we can test our model on the configurations we have not yet seen (ising_test_configs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(dense_model,test_loader)\n",
    "predictions = np.array([pred.numpy() for pred in predictions]).flatten()\n",
    "acc = np.mean(predictions==test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy on test set =', acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006650aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ising_predictions = dense_model.predict(ising_test_configs)\n",
    "print(ising_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c5459",
   "metadata": {},
   "source": [
    "To evaluate where the model predicts $T_c$ to be, we average the prediction for all the configurations for a given temperature. \n",
    "\n",
    "We also calculate the absolute value of the magnetization ($m=|\\sum \\sigma_i|$) for comparison, since we know that this is our order parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1449309",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temps = list(np.sort(list(set(ising_test_labels))))\n",
    "NT = len(Temps)\n",
    "phase1 = np.zeros(NT)\n",
    "phase2 = np.zeros(NT)\n",
    "points = np.zeros(NT)\n",
    "m = np.zeros(NT)\n",
    "lastT = 0.\n",
    "for i, T in enumerate(ising_test_labels):\n",
    "    j = Temps.index(T)\n",
    "    phase1[j]+=ising_predictions[i:i+1, 0][0]\n",
    "    phase2[j]+=ising_predictions[i:i+1, 1][0]\n",
    "    m[j] += abs(np.mean(ising_test_configs[i]))\n",
    "    points[j]+=1.\n",
    "\n",
    "for j in range(NT):\n",
    "    phase1[j] /= points[j]\n",
    "    phase2[j] /= points[j]\n",
    "    m[j] /= points[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.plot(Temps, phase1, 'b', label='ordered')\n",
    "plt.plot(Temps, phase2, 'r', label='disordered')\n",
    "plt.plot(Temps, m, 'g--', label=\"magnetization\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlim(1,3.5)\n",
    "plt.xlabel('T [J]')\n",
    "plt.ylabel('model prediction/magnetization')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be5e40",
   "metadata": {},
   "source": [
    "We can now estimate the location of the transition. Lets define this to be the location where our model's prediction drops to  0.5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f676f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (np.abs(phase1 - 0.5)).argmin()\n",
    "tc = Temps[index]\n",
    "\n",
    "print(\"Estimated Transition Temp =\", tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ec7db",
   "metadata": {},
   "source": [
    "The exact transition temperature in the thermodynamic limit is  2.2692 , so our result is not so bad considering finite size effects. If we look again at the above plot, we can see that the curves coincide relatively nicely with the average magnetization, this suggest that the network is indeed learning the magnetization, i.e. it is computing the magnetization and using it to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-contact",
   "metadata": {
    "id": "broadband-contact"
   },
   "source": [
    "# Example #2: Ising model with local constraints: Ising Gauge Theory (IGT)\n",
    "\n",
    "In the previous example, we classified spin configurations of the simple Ising model. That was a relatively easy task given that we know that there's a global order parameter, i.e., the magnetization that distinguishes the two phases the model has.\n",
    "\n",
    "In the following, we will look at spin configurations coming from a different model on which the simple PCA spectacularly fails. In this model, Ising spins live on the edges of a square lattice (see Figs. below). The Hamiltonian then favors even down and up spins around a square. If the number is odd, a pentalty is paid. The Hamiltonian is given by\n",
    "\n",
    "\\begin{align}\n",
    "H(\\boldsymbol{\\sigma}) = -\\sum_{p} \\prod_{i \\in p}\\sigma_{i},\n",
    "\\end{align}\n",
    "where we sum over the plaquettes $p$ of the square lattice.\n",
    "\n",
    "This model does not have a finite temperature transition. We thus want to train a network to distinguish the (highly degenerate) ground states of this system from any excited state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-saturday",
   "metadata": {
    "id": "overhead-saturday"
   },
   "source": [
    "First, we load and analyze the shape of our data set again. As before, they are located in the folder `Ising` and labeled by a temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-basic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "innovative-basic",
    "outputId": "b2e623c5-dde0-4357-bdc3-b654436a1ca1"
   },
   "outputs": [],
   "source": [
    "N = 16 # linear dimension of the lattice \n",
    "\n",
    "ilgt_training_configs = np.load(folder + \"/ilgt_training_configs.npy\".format(N))\n",
    "ilgt_training_labels = np.load(folder + \"/ilgt_training_labels.npy\".format(N))\n",
    "ilgt_test_configs = np.load(folder + \"/ilgt_test_configs.npy\".format(N))\n",
    "ilgt_test_labels = np.load(folder + \"/ilgt_test_labels.npy\".format(N))\n",
    "\n",
    "print('train_images.shape =', ilgt_training_configs.shape)\n",
    "print('train_labels.shape =', ilgt_training_labels.shape)\n",
    "print('test_images.shape =', ilgt_test_configs.shape)\n",
    "print('test_labels.shape =', ilgt_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap data in a Dataset instance\n",
    "train_set = TensorDataset(torch.tensor(ilgt_training_configs,dtype=torch.float),torch.tensor(ilgt_training_labels,dtype=torch.long))\n",
    "test_set  = TensorDataset(torch.tensor(ilgt_test_configs,dtype=torch.float),    torch.tensor(ilgt_test_labels,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build this dense neural network (DNN) ourselves now!\n",
    "## We want (for start) a DNN which takes an input of certain shape\n",
    "## then let's go with hidden layer of 100 neurons and ReLU\n",
    "## then output layer (# of neurons = # of classes in the problem)\n",
    "\n",
    "# Dense network\n",
    "#####################################################################\n",
    "### TODO: ###\n",
    "#####################################################################\n",
    "# implement the above described dense network.\n",
    "# You can also play around with the hyperparameters though\n",
    "# dense_network = ...\n",
    "\n",
    "# Print a summary of the model\n",
    "print(dense_network)\n",
    "\n",
    "# wrap in pytorch lighting\n",
    "# dense_model = ...\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "### TODO: ###\n",
    "#####################################################################\n",
    "# Now let's train our DNN! 50 epochs, take batch size 32\n",
    "# wrap the torch dataset instance in a data loader first\n",
    "# ...\n",
    "# create a trainer instance\n",
    "# ...\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the DNN perform?\n",
    "predictions = trainer.predict(dense_model,test_loader)\n",
    "predictions = np.array([pred.numpy() for pred in predictions]).flatten()\n",
    "acc = np.mean(predictions==ilgt_test_labels)\n",
    "\n",
    "print('Dense Network')\n",
    "print('accuracy on test set = {}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ff896",
   "metadata": {},
   "source": [
    "### WOW! What just happened? Can you guess by checking the training history? Plot the accuracy and loss for training and validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92838e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the training looked like:\n",
    "# summarize history for accuracy\n",
    "plot_history(dense_model, 'acc')\n",
    "# summarize history for loss function value\n",
    "plot_history(dense_model, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8e1c0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Finally, let's play with one of the most powerful ML models, designed especially for images (i.e., for higher-dimentional data where spatial dependence is a significant feature).\n",
    "\n",
    "What do convolutional layers do?\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/789/0*jLoqqFsO-52KHTn9.gif)\n",
    "\n",
    "The yellow matrix is called a kernel, and its size is one of the hyperparameters. It moves around the green (input) image with step defined by `stride` (here = 1), and how it behaves at the edges of the image is called `padding`. The resulting convolved image is an input to a next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd455f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to be able to use different kernel sizes and compare\n",
    "conv_network = nn.Sequential(\n",
    "    nn.Conv2d(2,16,kernel_size=2,stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(17*17*16,8),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(8,2)\n",
    ")\n",
    "\n",
    "conv_model = LitModel(conv_network)\n",
    "\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5a1fc",
   "metadata": {},
   "source": [
    "We have introduced here two new layers. Lets briefly understand what each layer does.\n",
    "\n",
    "1.  **Convolutional**: This layer applies 16 kernels of size 2 by 2 over the input image (in case they are identical, we can also only provide a single integer). For our purpose, we need periodic boundary conditions and we thus use no padding, which means it does not add additional 'pixels' around the configuration. We instead add the padding ourselves (see below).\n",
    "```\n",
    " nn.Conv2D(16, kernel_size=(2,2), strides=(1, 1), padding = 0)\n",
    "```\n",
    "\n",
    "\n",
    "2.  **Dropout**: This layers drops the nodes in the layer above with a 50% probability.\n",
    "```\n",
    "nn.Dropout(0.5)\n",
    "```\n",
    "Note that this layer is only applied during training. When evaluating or predicting on test samples, this layer is not applied.\n",
    "\n",
    "For more information about layer check out the keras documentation: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "\n",
    "One final note on the input shape: While for the (first) dense layer we can define just about any shape, the input shape for the convolutional layer is necessarily N x C x M, where C is the number of channels. If the input available has only a single channel, i.e., its shape is N x M, an additional axis with dimension 1 needs to be added for it to work (to make it N x 1 x M). M here can even be another shape M = M1 x M2 x ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the periodic padding for training and test configurations\n",
    "def create_periodic_padding(configs, kernel_size=2):\n",
    "    N = np.shape(configs)[1]\n",
    "    padding = kernel_size-1\n",
    "    x = []\n",
    "    for config in configs:\n",
    "        padded = np.zeros((N+2*padding, N+2*padding, 2))\n",
    "        # lower left corner\n",
    "        padded[:padding,:padding, :] = config[N-padding:,N-padding:,:]\n",
    "        # lower middle\n",
    "        padded[padding:N+padding, :padding, :] = config[:,N-padding:,:]\n",
    "        # lower right corner\n",
    "        padded[N+padding:, :padding, :] = config[:padding, N-padding:, :]\n",
    "        # left side\n",
    "        padded[:padding, padding:N+padding, :] = config[N-padding:, :, :]\n",
    "        # center\n",
    "        padded[padding:N+padding, padding:N+padding, :] = config[:,:,:]\n",
    "        # right side\n",
    "        padded[N+padding:, padding:N+padding, :] = config[:padding, :, :]\n",
    "        # top left corner\n",
    "        padded[:padding, N+padding:,:] = config[N-padding:, :padding, :]\n",
    "        # top middle\n",
    "        padded[padding:N+padding, N+padding:, :] = config[:, :padding, :]\n",
    "        # top right corner\n",
    "        padded[N+padding:, N+padding:, :] = config[:padding, :padding, :]\n",
    "        x.append(padded)\n",
    "    return np.array(x).transpose([0,3,1,2]) # this ensures that the color channel is at the right position\n",
    "\n",
    "x_n = create_periodic_padding(ilgt_training_configs)\n",
    "test_x_n = create_periodic_padding(ilgt_test_configs)\n",
    "\n",
    "# wrap data in a Dataset instance\n",
    "train_set = TensorDataset(torch.tensor(x_n,dtype=torch.float),torch.tensor(ilgt_training_labels,dtype=torch.long))\n",
    "test_set  = TensorDataset(torch.tensor(test_x_n,dtype=torch.float),    torch.tensor(ilgt_test_labels,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the distinctive shape now\n",
    "x_n.shape, test_x_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52804e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "### TODO: ###\n",
    "#####################################################################\n",
    "# Now let's train our conv model now\n",
    "# wrap the torch dataset instance in a data loader first\n",
    "# ...\n",
    "# create a trainer instance\n",
    "# ...\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c56a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Network\n",
    "predictions = trainer.predict(conv_model,test_loader)\n",
    "predictions = np.array([pred.numpy() for pred in predictions]).flatten()\n",
    "acc = np.mean(predictions==ilgt_test_labels)\n",
    "\n",
    "print('Conv Network')\n",
    "print('accuracy on test set = {}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc314605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the training looked like:\n",
    "# summarize history for accuracy\n",
    "plot_history(conv_model, 'acc')\n",
    "# summarize history for loss function value\n",
    "plot_history(conv_model, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de71a5",
   "metadata": {},
   "source": [
    "Comparing the plots for the dense network (above) and the conv network (here), we can observe a few things. \n",
    "\n",
    "1.  **Dense Network (Blue**):  Very quickly, the loss is decreasing on the training set, but it is actually increasing for the validation set. There is a large and widening gap between validation loss and training loss.  (We are overfitting)\n",
    "2.   **Convolutional Network (Red)**: Both the validation and the training losses are decreasing for all epochs (No overfitting).\n",
    "3. Even though the dense network has a lower loss on the training set, the loss on the validation set is much higher than the convolutional network.\n",
    "\n",
    "We can see that the dropout layer is definitely helps to prevent the overfitting.\n",
    "For many applications, adding a dropout layer can help avoiding overfitting. This is, however, not enough here - we also have two include conv layers as well!\n",
    "\n",
    "\n",
    "Some ways to combat overfitting:\n",
    "\n",
    "1. **Dropout layers**: This prevents the model from co-adapting or memorising data\n",
    "2. **Adding regularisation** terms to cost function, e.g. to penalise large parameters: For example, adding a L2 regularisation term to the loss function, i.e. L2 = ||w||^2 where w represents the weights of the corresponding layer. This limits the power of the model by preventing it from exploring large parameters. In pytorch, this can be done by adjusting the arguments of the *optimizer* accordingly, see https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "\n",
    "3. **Early stopping**: Here we simply stop training when we have achieved a satisfactory validation accuracy or loss. For example, in the dense network we trained above, it is probably a good idea to stop the training after aroud 11 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30334eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "innovative-change",
    "Nt7BH6snkeXi",
    "Xh_GCYflk3VH",
    "tJ50zvj2g1vZ"
   ],
   "name": "01_Unsupervised_learning_ST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
