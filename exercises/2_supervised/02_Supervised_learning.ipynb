{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "possible-truck",
   "metadata": {
    "id": "possible-truck"
   },
   "source": [
    "# Notebook 2: Classification with Neural Networks\n",
    "\n",
    "**Authors:** Kenny Choo, Mark H. Fischer, Eliska Greplova for the conference \"Summer School: ML in Quantum Physics and Chemistry\" (24.08.-03.09.2021, Warsaw)\n",
    "\n",
    "Adapted for the ML4Q-retreat 2022 by Alexander Gresch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9db9b",
   "metadata": {},
   "source": [
    "## Import of the first ML library!\n",
    "There are various libraries useful for ML practitioners. Among the most popular are Tensorflow (with Keras) and PyTorch. Both are pretty competitive with each other, so basically both offer the same functionalities [[source]](https://www.imaginarycloud.com/blog/pytorch-vs-tensorflow/).\n",
    "- **Tensorflow** is a Python numerical library (created by the Google Brain team) with many methods being in fact high-performance C++ binaries. It can be accelerated in the Google Colaboratory by choosing TPU (Tensor Programming Units). Keras is the high-level API of TensorFlow 2, so a kind of user-friendly overlay which simplifies many operations.\n",
    "- **PyTorch** is a Python library created by Facebook AI Research Lab. Before Tensorflow 2.0, PyTorch strength was a little different way of storing and building ML models, which allowed easier interaction with the models' internals. But Tensorflow 2.0 adapted to that and now the main difference is a little easier debugging of a PyTorch code than Tensorflow one.\n",
    "\n",
    "Fun fact to confuse you even more: Keras API used to build ML models in Tensorflow looks very similar to the PyTorch way of building ML models. We will stick to PyTorch. Note that there exists a wrapper module, pytorch lightning, for a more convenient control of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44K0-SShu7X3",
   "metadata": {
    "id": "44K0-SShu7X3"
   },
   "outputs": [],
   "source": [
    "# Helper Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rYL08R8jrUpF",
   "metadata": {
    "id": "rYL08R8jrUpF"
   },
   "source": [
    "## Step 0. Loading and saving files with Google Colaboratory\n",
    "\n",
    "Within these tutorials, we will need to upload some data (in particular, training data). E.g., today, we will upload Monte Carlo generated configurations of Ising spins on a 30x30 lattice. To upload data, you can pick one out of the three options (also, mix them freely):\n",
    "\n",
    "- **Option A**. *Downloading data onto your Google drive and mounting the Google drive to this Colab notebook.*\n",
    "\n",
    "\"Mounting\" means giving a temporary permission to a specific Colab notebook which enables browsing, loading, and saving files on your Google drive from this specific notebook. Once you exit the notebook, the perimission is gone, and you need to mount the Google drive again. Mounting is accompanied by an additional log-in to your Gmail account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eApxNcEOtqkH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eApxNcEOtqkH",
    "outputId": "7065ca30-35ea-4005-e07e-ea27a6c96a7d"
   },
   "outputs": [],
   "source": [
    "# Option A.\n",
    "# Mount your Google Drive:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g53rdXvzutr2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "g53rdXvzutr2",
    "outputId": "88e16bb5-1b5e-47d7-99b5-b45ad8f4f36b"
   },
   "outputs": [],
   "source": [
    "# Option A.\n",
    "# Folder where you store the data:\n",
    "folder = '/content/drive/MyDrive/Colab Notebooks/retreat/'\n",
    "\n",
    "# Access the data with simple commands:\n",
    "ising_training_configs = np.load(folder + \"/Ising/ising_training_configs_30x30.npy\")\n",
    "\n",
    "# You can also save them there:\n",
    "x = np.array([0,1])\n",
    "np.save(folder + \"/first_save\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A0Bc_uCRPsfG",
   "metadata": {
    "id": "A0Bc_uCRPsfG"
   },
   "source": [
    "- **Option B.** *Clone the GitHub repository*\n",
    "\n",
    "If you want to avoid working with Google drive, you can use this work-around. All needed data are on [this GitHub](https://github.com/Shmoo137/SummerSchool2021_MLinQuantum). You can clone this GitHub repository into your Colab environment in the same way as you would in your local machine, using git clone. Once the repository is cloned, refresh the file-explorer on the left to browse through its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DtMKirDKQLW3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtMKirDKQLW3",
    "outputId": "952d3967-983d-4eb8-f64d-604e9ea90ccc"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "!git clone https://github.com/GreschAl/ML4Q_retreat22_ML_with_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgorAcwwQwZJ",
   "metadata": {
    "id": "mgorAcwwQwZJ"
   },
   "outputs": [],
   "source": [
    "# Option B.\n",
    "folder = \"/content/ML4Q_retreat22_ML_with_python/exercises/Ising_data\"\n",
    "# Access the data with simple commands:\n",
    "ising_training_configs = np.load(folder + \"/ising_training_configs_30x30.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6L_cH6L3QL54",
   "metadata": {
    "id": "6L_cH6L3QL54"
   },
   "source": [
    "Take note these are local and temporary files, and will disappear after closing the notebook. For saving and loading data from your local machine, look at Option C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBzzVvg2tq6I",
   "metadata": {
    "id": "tBzzVvg2tq6I"
   },
   "source": [
    "- **Option C.** *Downloading data onto your local machine and loading/saving data onto your local computer*\n",
    "\n",
    "For saving and loading data from your local machine, you will need 'google.colab' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqFIeepqMdge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "iqFIeepqMdge",
    "outputId": "29614c9a-fa22-4716-8595-0c377f336737"
   },
   "outputs": [],
   "source": [
    "# Option C.\n",
    "from google.colab import files\n",
    "\n",
    "# Saving data on your local machine:\n",
    "x = np.array([0,1])\n",
    "np.save('trial', x)\n",
    "files.download('trial.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78op-DswNak3",
   "metadata": {
    "id": "78op-DswNak3"
   },
   "outputs": [],
   "source": [
    "# Option C.\n",
    "# Loading data from your local machine:\n",
    "files.upload()  # it opens the file explorer where you can pick which files to upload \n",
    "                # (names stay the same as local ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7niEuj98UGkc",
   "metadata": {
    "id": "7niEuj98UGkc"
   },
   "source": [
    "In this notebook we will repeat the exercise from the notebook `01_Unsupervised_learning` on using data analysis to try to identify different phases of matter. We have seen that sometimes clustering can be really powerful tool, but sometimes it does not work too well. Here, we will learn how to accomplish the same task using neural networks and see if they can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-minutes",
   "metadata": {
    "id": "reserved-minutes"
   },
   "source": [
    "# Example #1: Ising Spin Configuration Classification\n",
    "\n",
    "\n",
    "The Ising model is given by the (classical) Hamiltonian:\n",
    "\n",
    "\\begin{align}\n",
    "H(\\boldsymbol{\\sigma}) = -\\sum_{<ij>} \\sigma_{i}\\sigma_{j},\n",
    "\\end{align}\n",
    "where the spins $\\sigma_{i} \\in \\lbrace -1, 1 \\rbrace$ are binary variables living on the vertices of a square lattice and the sum is taken over nearest neighbours $<ij>$. We have set $J=1$.\n",
    "  \n",
    "At a given temperature $\\beta = 1/T$, the probability of a configuration $\\sigma$ is given by the Boltzmann distribution\n",
    "  \n",
    "\\begin{align}\n",
    "  P(\\boldsymbol{\\sigma}) = \\frac{e^{-\\beta H(\\boldsymbol{\\sigma})}}{Z},\n",
    " \\end{align}\n",
    "  \n",
    "  where $Z$ is the partition function. This model exhibits a phase transition from the ferromagnetic phase at low tempertures to a paramagnetic phase at high temperatures. The transition temperature is $T_c \\approx 2.2692$.\n",
    "  \n",
    "  **Task**\n",
    " \n",
    "1.   Classify the ferromagnetic versus the paramagnetic phase of the Ising model\n",
    "2.   Find the transition temperature\n",
    "  \n",
    "**Dataset**: Monte Carlo generated configurations on a 30x30 square lattice. The configuration are labelled by temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-norway",
   "metadata": {
    "id": "above-norway"
   },
   "source": [
    "## Step 1: Import data and analyze the data shape\n",
    "\n",
    "The folder `Ising_data` contains Monte Carlo generated Ising configurations on the two-dimensional lattice. The data set is divided into training and test parts and corresponding label files containing the temperature, $T$, of each Monte Carlo sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-stylus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "outstanding-stylus",
    "outputId": "a8643e4c-af7e-456b-8524-495eb81eeaac"
   },
   "outputs": [],
   "source": [
    "N = 30 # linear dimension of the lattice \n",
    "\n",
    "ising_training_configs = np.load(folder + \"/ising_training_configs_{0}x{0}.npy\".format(N))\n",
    "ising_training_labels = np.load(folder + \"/ising_training_labels_{0}x{0}.npy\".format(N))\n",
    "ising_test_configs = np.load(folder + \"/ising_test_configs_{0}x{0}.npy\".format(N))\n",
    "ising_test_labels = np.load(folder + \"/ising_test_labels_{0}x{0}.npy\".format(N))\n",
    "\n",
    "print('train_images.shape =', ising_training_configs.shape)\n",
    "print('train_labels.shape =', ising_training_labels.shape)\n",
    "print('test_images.shape =', ising_test_configs.shape)\n",
    "print('test_labels.shape =', ising_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-economy",
   "metadata": {
    "id": "sharp-economy"
   },
   "source": [
    "We see that we have a training set of size 1000 and a test set of size 1000.\n",
    "Each image is a 30x30 array which takes values in {-1, 1}. The labels of these images are the temperatures and they are in [1, 3.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-shock",
   "metadata": {
    "id": "foreign-shock"
   },
   "source": [
    "## Step 2: Prepare data\n",
    "\n",
    "At the moment, our configurations are labelled by their temperature. Since we want to learn to classify the two phases, we need to label our data by 'Ordered' (label=0) vs 'Disordered (label=1).\n",
    "\n",
    "Let us assume that we know $1.5=T_{low}<T_{c}< T_{high} = 2.5$. Then we exclude all the data between $T_{low}$ and $T_{high}$. We label all configurations below $T_{low}$ with '0' and all those above $T_{high}$ with '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-swedish",
   "metadata": {
    "id": "delayed-swedish"
   },
   "outputs": [],
   "source": [
    "# Assign labels according to the temperature\n",
    "T_low = 1.5\n",
    "T_high = 2.5\n",
    "\n",
    "########################################################################\n",
    "### TODO: ###\n",
    "########################################################################\n",
    "# Starting with training data, create the labels for the data according\n",
    "# to the explanation above. Do not forget to omit data from inbetween the phases.\n",
    "included = np.bitwise_or(ising_training_labels <= T_low , ising_training_labels >= T_high)\n",
    "train_images = ising_training_configs[included]\n",
    "train_labels = np.zeros(len(train_images),dtype=int)\n",
    "train_labels[ising_training_labels[included] >= T_high] = 1\n",
    "\n",
    "# do the same for the test data\n",
    "included = np.bitwise_or(ising_test_labels <= T_low , ising_test_labels >= T_high)\n",
    "test_images = ising_test_configs[included]\n",
    "test_labels = np.zeros(len(test_images),dtype=int)\n",
    "test_labels[ising_test_labels[included] >= T_high] = 1\n",
    "########################################################################\n",
    "\n",
    "# Now you should have smaller training data set, check it:\n",
    "print(train_images.shape,train_labels.shape)\n",
    "# Now you should have smaller training data set, check it:\n",
    "print(test_images.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b1a53",
   "metadata": {},
   "source": [
    "## Step 3: Setup the model\n",
    "\n",
    "Specifications of our model: \n",
    "\n",
    "*  Our model takes in a 30 by 30 array\n",
    "*  And outputs a 2-dimensional vector\n",
    "\n",
    "The 2-dimensional vector gives the models prediction for whether the system is ferromagnetic (i.e., $T < T_c$) or paramagnetic ($T>T_c$).\n",
    "\n",
    "The model's prediction is given by the index with the largest value, i.e. argmax(output)\n",
    "\n",
    "There are two ways to create models within pytorch.\n",
    "\n",
    "1.   Sequential Model\n",
    "2.   Model class with the functional API\n",
    "\n",
    "In both methods, the basic building block is the layer. A layer takes some input tensor and applies some transformation and returns an output tensor.\n",
    "\n",
    "First let us explore the sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(30, 30)),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# or equivalently\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(30, 30)))\n",
    "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "# We can also print a summary of our model by\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8f05d",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Why 32 neurons? Good question! No one knows exactly ;) Building a ML model is a combination of educated guess, good practices, and pure luck. General rule - the simpler model (the smaller number of parameters), the better. We will encounter more quantities on whose values we need to decide to some degree arbitrarily. These quantities are called **hyperparameters**.\n",
    "\n",
    "*Aside note for curious minds:*\n",
    "\n",
    "While training, you optimize the parameters of the model. You check the performance of the trained model (on the subset of data called validation data) and then try to improve it by playing with hyperparameters. You can think that you optimize the model parameters on the training data and the hyperparameters on the validation data. The final test of the model performance is the test data. \n",
    "\n",
    "*Question:* Why shouldn't you play with your hyperparameters to improve your model performance on test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec89a5",
   "metadata": {},
   "source": [
    "## Step 3. Compile the model\n",
    "Now that the model is defined, we need to train our model. But before doing so, there are a few details we need to specify.\n",
    "\n",
    "\n",
    "1.   Loss function: we need to choose what function we want our model to minimise e.g. mean square error or cross entropy or ...\n",
    "2.   Optimisation method: How we want to update the weights e.g. stochastic gradient descent or ADAM or ...\n",
    "3.   Metrics: some quantity we want to keep track off while we are training, e.g. value of the loss function or the accuracy of the model...\n",
    "\n",
    "One could also choose other loss functions or optimisers: https://keras.io/losses/, https://keras.io/optimizers/\n",
    "\n",
    "It is always good to check the history of the training. How the training loss looked like? How the validation loss looked like? You can learn a lot from that and we will show you a very meaningful example in the end of this notebook. For now, remember it is a good practice to plot the training and validation loss!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8854e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, this part would take more lines ;)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee496b2f",
   "metadata": {},
   "source": [
    "## Step 4. Train the model\n",
    "\n",
    "Finally we are ready to train our model. Here we basically just need to feed the data set to our model, and the model would minimise the loss function we chose and update the model parameters according to the optimisation method we specified. \n",
    "\n",
    "The last thing we need to choose is the number of epochs and the batch_size.\n",
    "\n",
    "\n",
    "*   batch_size: this is the number of images we feed to our model in 1 iteration\n",
    "*   epochs: the number of times we run through our data set.\n",
    "\n",
    "Let's suppose batch size = 100. Then the training proceeds as follows: \n",
    "\n",
    "1.   Divide our dataset in batches of 100. \n",
    "2.   Take a batch of 100 samples and feed it to the model. This gives 100 output vectors from which we compute the loss function and its gradient w.r.t. to model parameters.\n",
    "3. Use the gradients to update the model parameters according to the optimiser we chose \n",
    "4. Repeat steps 2 and 3 until we have cycled through to the end of the dataset. This will be the end of one epoch.\n",
    "\n",
    "Number of iterations in one epoch = size of data set / batch_size\n",
    "\n",
    "\n",
    "A few comments:\n",
    "\n",
    "* The smaller the batch size, the faster the model trains.\n",
    "* The smaller the batch size, the more noisy the training will be.\n",
    "* Some amount of noise is useful to prevent us from getting stucked in local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef182617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful plotting procedure to plot training and validation losses vs. training time (measured in so-called epochs)\n",
    "def plot_history(histories, key='sparse_categorical_crossentropy'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ising_dense_history = model.fit(train_images, train_labels, epochs = 10, batch_size = 16, \n",
    "                                validation_data=(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the training looked like:\n",
    "print(ising_dense_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plot_history([('Dense Neural Network for Ising model', ising_dense_history)], 'accuracy')\n",
    "# summarize history for loss function value\n",
    "plot_history([('Dense Neural Network for Ising model', ising_dense_history)], 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e297c9",
   "metadata": {},
   "source": [
    "### Can you tell something out of these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a02b35",
   "metadata": {},
   "source": [
    "## Step 5. Evaluate our model\n",
    "Now that our model is trained, we can test our model on the configurations we have not yet seen (ising_test_configs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('loss on test set =', loss)\n",
    "print('accuracy on test set =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006650aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(ising_test_configs)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c5459",
   "metadata": {},
   "source": [
    "To evaluate where the model predicts $T_c$ to be, we average the prediction for all the configurations for a given temperature. \n",
    "\n",
    "We also calculate the absolute value of the magnetization ($m=|\\sum \\sigma_i|$) for comparison, since we know that this is our order parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1449309",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temps = list(np.sort(list(set(ising_test_labels))))\n",
    "NT = len(Temps)\n",
    "phase1 = np.zeros(NT)\n",
    "phase2 = np.zeros(NT)\n",
    "points = np.zeros(NT)\n",
    "m = np.zeros(NT)\n",
    "lastT = 0.\n",
    "for i, T in enumerate(ising_test_labels):\n",
    "    j = Temps.index(T)\n",
    "    phase1[j]+=prediction[i:i+1, 0][0]\n",
    "    phase2[j]+=prediction[i:i+1, 1][0]\n",
    "    m[j] += abs(np.mean(ising_test_configs[i]))\n",
    "    points[j]+=1.\n",
    "\n",
    "for j in range(NT):\n",
    "    phase1[j] /= points[j]\n",
    "    phase2[j] /= points[j]\n",
    "    m[j] /= points[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.plot(Temps, phase1, 'b', label='ordered')\n",
    "plt.plot(Temps, phase2, 'r', label='disordered')\n",
    "plt.plot(Temps, m, 'g--', label=\"magnetization\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlim(1,3.5)\n",
    "plt.xlabel('T [J]')\n",
    "plt.ylabel('model prediction/magnetization')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be5e40",
   "metadata": {},
   "source": [
    "We can now estimate the location of the transition. Lets define this to be the location where our model's prediction drops to  0.5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f676f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (np.abs(phase1 - 0.5)).argmin()\n",
    "tc = Temps[index]\n",
    "\n",
    "print(\"Estimated Transition Temp =\", tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ec7db",
   "metadata": {},
   "source": [
    "The exact transition temperature in the thermodynamic limit is  2.2692 , so our result is not so bad considering finite size effects. If we look again at the above plot, we can see that the curves coincide relatively nicely with the average magnetization, this suggest that the network is indeed learning the magnetization, i.e. it is computing the magnetization and using it to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-contact",
   "metadata": {
    "id": "broadband-contact"
   },
   "source": [
    "# Example #2: Ising model with local constraints: Ising Gauge Theory (IGT)\n",
    "\n",
    "In the previous example, we classified spin configurations of the simple Ising model. That was a relatively easy task given that we know that there's a global order parameter, i.e., the magnetization that distinguishes the two phases the model has.\n",
    "\n",
    "In the following, we will look at spin configurations coming from a different model on which the simple PCA spectacularly fails. In this model, Ising spins live on the edges of a square lattice (see Figs. below). The Hamiltonian then favors even down and up spins around a square. If the number is odd, a pentalty is paid. The Hamiltonian is given by\n",
    "\n",
    "\\begin{align}\n",
    "H(\\boldsymbol{\\sigma}) = -\\sum_{p} \\prod_{i \\in p}\\sigma_{i},\n",
    "\\end{align}\n",
    "where we sum over the plaquettes $p$ of the square lattice.\n",
    "\n",
    "This model does not have a finite temperature transition. We thus want to train a network to distinguish the (highly degenerate) ground states of this system from any excited state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-saturday",
   "metadata": {
    "id": "overhead-saturday"
   },
   "source": [
    "First, we load and analyze the shape of our data set again. As before, they are located in the folder `Ising` and labeled by a temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-basic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "innovative-basic",
    "outputId": "b2e623c5-dde0-4357-bdc3-b654436a1ca1"
   },
   "outputs": [],
   "source": [
    "N = 16 # linear dimension of the lattice \n",
    "\n",
    "ilgt_training_configs = np.load(folder + \"/ilgt_training_configs.npy\".format(N))\n",
    "ilgt_training_labels = np.load(folder + \"/ilgt_training_labels.npy\".format(N))\n",
    "ilgt_test_configs = np.load(folder + \"/ilgt_test_configs.npy\".format(N))\n",
    "ilgt_test_labels = np.load(folder + \"/ilgt_test_labels.npy\".format(N))\n",
    "\n",
    "print('train_images.shape =', ilgt_training_configs.shape)\n",
    "print('train_labels.shape =', ilgt_training_labels.shape)\n",
    "print('test_images.shape =', ilgt_test_configs.shape)\n",
    "print('test_labels.shape =', ilgt_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build this dense neural network (DNN) ourselves now!\n",
    "## We want (for start) a DNN which takes an input of certain shape\n",
    "## then let's go with hidden layer of 100 neurons and ReLU\n",
    "## then output layer (# of neurons = # of classes in the problem) with Softmax\n",
    "\n",
    "# Dense network\n",
    "dense = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(16,16,2)),\n",
    "    keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model with an Adam optimiser\n",
    "# Take sparse categorical cross entropy as a loss function\n",
    "# And choose metrics (measures of model performace), e.g., accuracy and loss function above\n",
    "dense.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy','sparse_categorical_crossentropy']) \n",
    "\n",
    "# Print a summary of the model\n",
    "dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our DNN! 50 epochs, take batch size 32\n",
    "dense_history = dense.fit(ilgt_training_configs, ilgt_training_labels, epochs = 50, batch_size = 32, \n",
    "                                validation_data=(ilgt_test_configs,ilgt_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the DNN performs?\n",
    "loss,acc,_ = dense.evaluate(ilgt_test_configs,ilgt_test_labels)\n",
    "\n",
    "print('Dense Network')\n",
    "print('loss on test set =', loss)\n",
    "print('accuracy on test set = {}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ff896",
   "metadata": {},
   "source": [
    "### WOW! What just happened? Can you guess by checking the training history? Plot the accuracy and loss for training and validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92838e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the training looked like:\n",
    "plt.plot(dense_history.history[\"loss\"],label=\"Train\")\n",
    "plt.plot(dense_history.history[\"val_loss\"],label=\"Val\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(dense_history.history[\"accuracy\"],label=\"Train\")\n",
    "plt.plot(dense_history.history[\"val_accuracy\"],label=\"Val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8e1c0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Finally, let's play with one of the most powerful ML models, designed especially for images (i.e., for higher-dimentional data where spatial dependence is a significant feature).\n",
    "\n",
    "What do convolutional layers do?\n",
    "\n",
    "![Alt Text](https://miro.medium.com/max/789/0*jLoqqFsO-52KHTn9.gif)\n",
    "\n",
    "The yellow matrix is called a kernel, and its size is one of the hyperparameters. It moves around the green (input) image with step defined by `stride` (here = 1), and how it behaves at the edges of the image is called `padding`. The resulting convolved image is an input to a next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd455f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to be able to use different kernel sizes and compare\n",
    "def convolutional_model(kernel_size):\n",
    "    input_size = 16+2*(kernel_size-1)\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(16, (kernel_size, kernel_size), strides=(1,1), padding='Valid', input_shape=(input_size,input_size,2), activation='relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "                  metrics=['accuracy','sparse_categorical_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601821f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 2\n",
    "model = convolutional_model(kernel_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5a1fc",
   "metadata": {},
   "source": [
    "We have introduced here two new layers. Lets briefly understand what each layer does.\n",
    "\n",
    "1.  **Convolutional**: This layer applies 32 kernels of size 2 by 2 over the input image. There are 2 paddings one can choose from 'valid' or 'same'. For our purpose, we need periodic boundary conditions and we thus use 'Valid', which means it does not add additional 'pixels' around the configuration. We instead add the padding ourselves (see below).\n",
    "```\n",
    " keras.layers.Conv2D(32, kernel_size=(2,2), strides=(1, 1), padding = 'Valid', activation=tf.nn.relu)\n",
    "```\n",
    "\n",
    "\n",
    "2.  **Dropout**: This layers drops the nodes in the layer above with a 50% probability.\n",
    "```\n",
    "keras.layers.Dropout(0.5)\n",
    "```\n",
    "Note that this layer is only applied during training. When evaluating or predicting on test samples, this layer is not applied.\n",
    "\n",
    "For more information about layer check out the keras documentation: https://keras.io/layers/about-keras-layers/\n",
    "\n",
    "\n",
    "One final note on the input shape: While for the (first) dense layer we can define just about any shape, the input shape for the convolutional layer is necessarily N x M x C, where C is the number of channels. If the input available has only a single channel, i.e., its shape is N x M, an additional axis with dimension 1 needs to be added for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8610ec56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ilgt_training_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed5c16fecfd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mx_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_periodic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milgt_training_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtest_x_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_periodic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milgt_test_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ilgt_training_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the periodic padding for training and test configurations\n",
    "def create_periodic_padding(configs, kernel_size):\n",
    "    N = np.shape(configs)[1]\n",
    "    padding = kernel_size-1\n",
    "    x = []\n",
    "    for config in configs:\n",
    "        padded = np.zeros((N+2*padding, N+2*padding, 2))\n",
    "        # lower left corner\n",
    "        padded[:padding,:padding, :] = config[N-padding:,N-padding:,:]\n",
    "        # lower middle\n",
    "        padded[padding:N+padding, :padding, :] = config[:,N-padding:,:]\n",
    "        # lower right corner\n",
    "        padded[N+padding:, :padding, :] = config[:padding, N-padding:, :]\n",
    "        # left side\n",
    "        padded[:padding, padding:N+padding, :] = config[N-padding:, :, :]\n",
    "        # center\n",
    "        padded[padding:N+padding, padding:N+padding, :] = config[:,:,:]\n",
    "        # right side\n",
    "        padded[N+padding:, padding:N+padding, :] = config[:padding, :, :]\n",
    "        # top left corner\n",
    "        padded[:padding, N+padding:,:] = config[N-padding:, :padding, :]\n",
    "        # top middle\n",
    "        padded[padding:N+padding, N+padding:, :] = config[:, :padding, :]\n",
    "        # top right corner\n",
    "        padded[N+padding:, N+padding:, :] = config[:padding, :padding, :]\n",
    "        x.append(padded)\n",
    "    return np.array(x)\n",
    "\n",
    "x_n = create_periodic_padding(ilgt_training_configs,kernel_size)\n",
    "test_x_n = create_periodic_padding(ilgt_test_configs, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52804e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional_history = model.fit(x_n,ilgt_training_labels,epochs=50, \n",
    "                                  validation_data=(test_x_n, ilgt_test_labels), \n",
    "                                  batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c56a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Network\n",
    "loss, acc, _ = model.evaluate(test_x_n, ilgt_test_labels, verbose=0)\n",
    "print('Convolutional Network with dropout')\n",
    "print('loss on test set =', loss)\n",
    "print('accuracy on test set = {}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc314605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('Dense',dense_history),\n",
    "              ('Convolutional', convolutional_history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de71a5",
   "metadata": {},
   "source": [
    "From the above plot, we can observe a few things. \n",
    "\n",
    "1.  **Dense Network (Blue**):  Very quickly, the loss is decreasing on the training set, but it is actually increasing for the validation set. There is a large and widening gap between validation loss and training loss.  (We are overfitting)\n",
    "2. **Dense Network with dropout (green)**  By adding a dropout layer, one can see that the gap between the training and validation loss is smaller. For many applications, adding a dropout layer can help avoiding overfitting. This is, however, not enough here.\n",
    "2.   **Convolutional Network (Red)**: Both the validation and the training losses are decreasing for all epochs (No overfitting).\n",
    "3. Even though the dense network has a lower loss on the training set, the loss on the validation set is much higher than the convolutional network.\n",
    "\n",
    "We can see that the dropout layer is definitely helps to prevent the overfitting.\n",
    "\n",
    "Some ways to combat overfitting:\n",
    "\n",
    "1. **Dropout layers**: This prevents the model from co-adapting or memorising data\n",
    "2. **Adding regularisation** terms to cost function, e.g. to penalise large parameters: This limits the power of the model by preventing it from exploring large parameters, e.g.\n",
    "```\n",
    "l2_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.001), activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "])\n",
    "```\n",
    "This would add a L2 regularisation term to the loss function, i.e. L2 = ||w||^2 where w represents the weights of the corresponding layer.\n",
    "\n",
    "\n",
    "3. **Early stopping**: Here we simply stop training when we have achieved a satisfactory validation accuracy or loss. For example, in the dense network we trained above, it is probably a good idea to stop the training after aroud 11 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30334eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "innovative-change",
    "Nt7BH6snkeXi",
    "Xh_GCYflk3VH",
    "tJ50zvj2g1vZ"
   ],
   "name": "01_Unsupervised_learning_ST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
